# CS231


[TOC]


## 1: Image Classification

Image Classification: A core task in Computer Vision 

The Problem: Semantic Gap 

Challenges

- viewpoint variation (角度变化)
- scale variation(尺寸差异)
- Illumination(照明)
- deformation(变形)
- Occlusion(遮挡)
- Background Clutter(背景杂斑)
- Intraclass variation (同属混杂)

**summary**

no obvious way to hard-code the algorithm for recognizing a cat, or other classes.

**解决办法:** Data-driven approach

输入数据 是 Width x Height x 3 的一个矩阵,3代表三信道.channel 数量.也有可能是2

### The image classification pipeline. 
We’ve seen that the task in Image Classification is to take an array of pixels that represents a single image and assign a label to it. Our complete pipeline can be formalized as follows:

- Input: Our input consists of a set of N images, each labeled with one of K different classes. We refer to this data as the training set.
- Learning: Our task is to use the training set to learn what every one of the classes looks like. We refer to this step as training a classifier, or learning a model.
- Evaluation: In the end, we evaluate the quality of the classifier by asking it to predict labels for a new set of images that it has never seen before. We will then compare the true labels of these images to the ones predicted by the classifier. Intuitively, we’re hoping that a lot of the predictions match up with the true answers (which we call the ground truth).

翻译

- Input:N张带标签的图片作为训练集
- Learning:
- Evaluation:


## KNN

- First classifier: **Nearest Neighbor**
- Example Dataset: **CIFAR10**
- Distance Metric to compare images

最近邻算法,KNN,K=1的特例

```python
import numpy as np
class NearestNeighbor(object):
    def __init__(self):
        pass

    def train(self, X, y):
        """

        :param X: N*D,each row is an example
        :param y: 1-dimension of size N
        :return:
        """
        self.Xtr = X
        self.ytr = y

    def predict(self, X):
        num_test = X.shape[0]
        Ypred = np.zeros(num_test, dtype=self.ytr.dtype)

        for i in range(num_test):
            distances = np.sum(np.abs(self.Xtr - X[i, :]), axis=1)
            min_index = np.argmin(distances)
            Ypred[i] = self.ytr[min_index]
        return Ypred

```

这里用的其实曼哈顿距离,另外一种距离计算方式是欧几里得距离.

- 曼哈顿距离,L1 distance,各维度差值的绝对值之和
- 欧几里得距离,L2 distance,各维度差值平方求和开根号.

具体公式如下:

![QQ20170511-132204.png](https://ooo.0o0.ooo/2017/05/11/5913f51b8d05a.png)

[链接]( http://vision.stanford.edu/teaching/cs231n-demos/knn/) 演示不同距离,不同K值下,knn算法的不同结果.

这里距离计算方式(distance metric),k值的大小,就是**超参数**

超参数选择:

- very problem-dependent,基于具体问题
- Must try them all out and see what works best,无脑试.


k-Nearest Neighbor on images **never used**.

- Very slow at test time
- Distance metrics on pixels are not informative
- curse of dimensionality

### K-Nearest Neighbors: Summary

- In **Image classification** we start with a **training set** of images and labels, and must predict labels on the **test set**
- The **K-Nearest Neighbors** classifier predicts labels based on nearest training examples
- Distance metric and K are **hyperparameters**
- Choose hyperparameters using the **validation set**; only run on the test set once at the very end!





## 2. Linear Classification


$$ f(x,W) = Wx + b $$

- W是对应的参数或者权重,shape为:10*3072
- x是一个一维向量,(32*32*3个数字组成),shape为:3072*1
- b,偏置(bias vector)shape为:10*1


线性模型是 NN 和CNN的基础






示例如下:
![QQ20170511-151555.png](https://ooo.0o0.ooo/2017/05/11/59140fd768ffb.png)

做了一个$R^D$到$R^K$映射,D是数据的维度,对于图片就是weith\*height\*channelNum,
K是图片的标签数量,每个值代表对应种类的得分.















### 理解线性模型

每张图片映射到高维空间,相当于一个点,一个种类的计算,就相当于一个分类超平面,也就是W的一列就是一个分类器.
K个种类的线性模型,对应K个超平面,K分分类器

![pixelspace.jpeg](https://ooo.0o0.ooo/2017/05/13/5916488379e4a.jpeg)



**另一种理解方式**:

W的一列提供了一种分类的 一个模板(template)、原型(prototype),每次检验一张图片和某个种类的模板的匹配程度,(做向量点积).
下面是**cifar10**的例子:
![templates.jpg](https://ooo.0o0.ooo/2017/05/13/59164a1b390d1.jpg)



**Bias trick**
把偏置b,合并到W里,x增加一个1,即可.
略.


### LOSS FUNCTION
损失函数
也叫
the cost function or the objective,代价函数,目标函数

#### Multiclass Support Vector Machine (SVM) loss

每个输入x,有K个得分 分别为$s_i,i=1,2,...K$,

第i个图片的真实lable为$y_i$

理想情况下:
$$若j \neq y_i,则 s_j - s_{y_i}+  \Delta \leq 0$$

由此定义Multiclass Support Vector Machine loss:

$$L_i = \sum_{j \neq y_i } {max(0, s_j - s_{y_i} + \Delta)}$$


结合线性分类的函数svm loss可以写为:

$$L_i = \sum_{j \neq y_i } {max(0, W_j^Tx - W_{y_i}^Tx + \Delta)}$$

$W_j^T$是矩阵$W^T$的第j列,也就是$W$的第j行,

- $max(0,-)$常被成为**hinge loss**
- $max(0,-)^2$,square hinge loss or L2-SVM loss
- 此处loss 是用来penalizes violated margins,即 对违反margin的度量.




#### Regularization
能使loss function为0,的W可能不止一个,

最一般的,若$W$满足$L_i=0$(for all i),那$\lambda W ( \lambda > 1)$显然也满足
因此 我们增加一个罚项$R(W)$,最常见的是$L2$,形式如下:
$$R(W) = \sum_k\sum_l W_{k,l}^2$$
罚项的计算只和权重有关,与数据集无关.


两部分损失合并

- data loss,$L_i$的均值
- regularization loss,权重的罚项


$$L = \frac 1{N}\sum L_i + \lambda R(W)$$

$$L = \frac 1{N}\sum_i\sum_{i\neq y_i}[max(0,f(x_i;W)_j - f(x_i:W)_{y_i} + \Delta)] + \lambda \sum_k\sum_lW^2_{k,l}$$

此处的$\lambda$又是一个超参数,一般由CV决定.

The most appealing property is that penalizing large weights tends to improve generalization, because it means that no input dimension can have a very large influence on the scores all by itself. 

罚项有助于提高泛化能力.



```python
def L_i(x, y, W):
    """
    unvectorized version. Compute the multiclass svm loss for a single example (x,y)
    - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)
    with an appended bias dimension in the 3073-rd position (i.e. bias trick)
    - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)
    - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)
    """
    delta = 1.0
    scores = W.dot(x)  # scores becomes of size 10 x 1, the scores for each class
    correct_class_score = scores[y]
    D = W.shape[0]  # number of classes, e.g. 10
    loss_i = 0.0
    for j in range(D): # iterate over all wrong classes
        if j == y:
          # skip for the true class to only loop over incorrect classes
          continue
        # accumulate loss for the i-th example
        loss_i += max(0, scores[j] - correct_class_score + delta)
    return loss_i

def L_i_vectorized(x, y, W):
    """
    只有一个循环,遍历所有样本点,其他都是向量化的,所以叫半向量化.
    A faster half-vectorized implementation. half-vectorized
    refers to the fact that for a single example the implementation contains
    no for loops, but there is still one loop over the examples (outside this function)
    """
    delta = 1.0
    scores = W.dot(x)
    # compute the margins for all classes in one vector operation
    margins = np.maximum(0, scores - scores[y] + delta)
    # on y-th position scores[y] - scores[y] canceled and gave delta. We want
    # to ignore the y-th position and only consider margin on max wrong class
    margins[y] = 0
    loss_i = np.sum(margins)
    return loss_i

def L(X, y, W):
    """
    fully-vectorized implementation :
    - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)
    - y is array of integers specifying correct class (e.g. 50,000-D array)
    - W are weights (e.g. 10 x 3073)
    """
    delta = 1.0
    scores = W.dot(X.T)
    lidx = range(0, len(y))


    margins = np.maximum(0, scores - scores[y, lidx] + delta)


    margins[y, lidx] = 0
    loss = np.sum(margins)/(len(y))
    return loss
```



#### 注意事项

$\Delta$ 和 $\lambda$ :

- $\Delta$ 常设为1 
- $\Delta$ 和 $\lambda$,的设置是目标函数 对于 data loss 和 regularization loss的trade off
- 与二分类svm,超参数的关系:$C 正比于 \frac1{\lambda}$


### softmax classifier

形如 $f_j(z) = \frac {e^{z_j}}{\sum_k e^{z_k}}$ 就叫**softmax function**

就是逻辑回归推广到多类别.
返回的不是分数高低,而是更加直观的概率大小

使用**cross-entropy loss** 而不是 hinge loss:

$$ L_i = -log(\frac {e^{f_{y_i}}}{\sum_j e^{f_j}})   $$
$$即:     L_i = -f_{y_i} + log\sum_je^{f_j} $$

$f$,就是x到结果的投影函数:$f(x_i;W) = Wx_i$.







___


## 3.Optimization



### optimization

loss function是用来衡量权重$W$的好坏的,optimization(优化)的作用就是找到一个$W$使loss function最小化.

此处以svm loss为例,这是一个凸函数,比较好优化,但在神经网络(NN)问题中,这些手段并不适用.





#### Strategy 1:Random  search

从若干随机权重中,选出表现最好的一个.(**brain-dead**)

``` python
# assume X_train is the data where each column is an example (e.g. 3073 x 50,000)
# assume Y_train are the labels (e.g. 1D array of 50,000)
# assume the function L evaluates the loss function

bestloss = float("inf") # Python assigns the highest possible float value
for num in range(100):
    W = np.random.randn(10, 3073) * 0.0001 # generate random parameters
    loss = L(X_train, Y_train, W) # get the loss over the entire training set
    if loss < bestloss: # keep track of the best solution
        bestloss = loss
        bestW = W
print('in attempt %d the loss was %f, best %f' % (num, loss, bestloss))
```





#### strategy 2:Random Local Search

随机下降法,从随机$W$开始随机迭代优化,核心是$W+\delta W$

```python
W = np.random.randn(10, 3073) * 0.001 # generate random starting W
bestloss = float("inf")
for i in xrange(1000):
  step_size = 0.0001
  Wtry = W + np.random.randn(10, 3073) * step_size
  loss = L(Xtr_cols, Ytr, Wtry)
  if loss < bestloss:
    W = Wtry
    bestloss = loss
  print('iter %d loss is %f' % (i, bestloss))
```







#### strategy 3:Following the Gradient

梯度下降法.我在别处证明过了,此处省略



### Computing the gradient

```python
# 不要拿int的array来测试这个函数
def eval_numerical_gradient(f, x):
    """ 
    a naive implementation of numerical gradient of f at x 
    - f should be a function that takes a single argument
    - x is the point (numpy array) to evaluate the gradient at
    """ 

    fx = f(x) # evaluate function value at original point
    grad = np.zeros(x.shape)
    h = 0.00001

    # iterate over all indexes in x
    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
    while not it.finished:

        # evaluate function at x+h
        ix = it.multi_index
        old_value = x[ix]
        x[ix] = old_value + h # increment by h
        fxh = f(x) # evalute f(x + h)
        x[ix] = old_value # restore to previous value (very important!)

        # compute the partial derivative
        grad[ix] = (fxh - fx) / h # the slope
        it.iternext() # step to next dimension

    return grad
```





### Gradient descent



### Summary











 





