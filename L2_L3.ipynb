{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches.meta data_batch_2 data_batch_4 readme.html\r\n",
      "data_batch_1 data_batch_3 data_batch_5 test_batch\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./cifar-10-batches-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = './cifar-10-batches-py/test_batch'\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo,encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "dd = unpickle(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'labels', b'data', b'filenames', b'batch_label'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw = dd[b'data']\n",
    "ydata = dd[b'labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L(X, y, W):\n",
    "    \"\"\"\n",
    "    fully-vectorized implementation :\n",
    "    - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)\n",
    "    - y is array of integers specifying correct class (e.g. 50,000-D array)\n",
    "    - W are weights (e.g. 10 x 3073)\n",
    "    \"\"\"\n",
    "    delta = 1.0\n",
    "    scores = W.dot(X.T)\n",
    "    lidx = range(0, len(y))\n",
    "    margins = np.maximum(0, scores - scores[y, lidx] + delta)\n",
    "    margins[y, lidx] = 0\n",
    "    loss = np.sum(margins)/(len(y))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 164 ms per loop\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xdata =[]\n",
    "for x in raw:\n",
    "    xdata.append(np.insert(x,0,1))\n",
    "xdata = np.array(xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# 每个样本增加一个bias 1\n",
    "\n",
    "# 法一\n",
    "# xdata =[]\n",
    "# for x in raw:\n",
    "#     xdata.append(np.insert(x,0,1))\n",
    "# xdata = np.array(xdata)\n",
    "\n",
    "#法二\n",
    "xdata = np.column_stack((raw, np.ones(len(raw))))\n",
    "\n",
    "# 法三\n",
    "# xdata = np.ones((raw.shape[0],raw.shape[1]+1))\n",
    "# xdata[:,:-1] = raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 3073), (10000, 3072))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdata.shape,raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.081358115448854"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.random.randn(10, 3073) * 0.0001\n",
    "\n",
    "L(xdata,ydata,W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = xdata\n",
    "Y_train = ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in attempt 99 the loss was 8.983980, best 8.983980\n"
     ]
    }
   ],
   "source": [
    "# 随机搜索\n",
    "bestloss = float(\"inf\") # Python assigns the highest possible float value\n",
    "for num in range(100):\n",
    "    W = np.random.randn(10, 3073) * 0.0001 # generate random parameters\n",
    "    loss = L(X_train, Y_train, W) # get the loss over the entire training set\n",
    "    if loss < bestloss: # keep track of the best solution\n",
    "        bestloss = loss\n",
    "        bestW = W\n",
    "print('in attempt %d the loss was %f, best %f' % (num, loss, bestloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 loss is 41.161338\n",
      "iter 1 loss is 41.161338\n",
      "iter 2 loss is 41.161338\n",
      "iter 3 loss is 41.104320\n",
      "iter 4 loss is 41.075899\n",
      "iter 5 loss is 41.075899\n",
      "iter 6 loss is 40.861117\n",
      "iter 7 loss is 40.679085\n",
      "iter 8 loss is 40.640761\n",
      "iter 9 loss is 40.521532\n",
      "iter 10 loss is 40.521532\n",
      "iter 11 loss is 40.521532\n",
      "iter 12 loss is 40.442019\n",
      "iter 13 loss is 40.442019\n",
      "iter 14 loss is 40.442019\n",
      "iter 15 loss is 40.216925\n",
      "iter 16 loss is 40.216925\n",
      "iter 17 loss is 40.104837\n",
      "iter 18 loss is 40.104837\n",
      "iter 19 loss is 40.104837\n",
      "iter 20 loss is 40.104837\n",
      "iter 21 loss is 40.000980\n",
      "iter 22 loss is 40.000980\n",
      "iter 23 loss is 39.901984\n",
      "iter 24 loss is 39.901984\n",
      "iter 25 loss is 39.783670\n",
      "iter 26 loss is 39.783670\n",
      "iter 27 loss is 39.577417\n",
      "iter 28 loss is 39.564077\n",
      "iter 29 loss is 39.546214\n",
      "iter 30 loss is 39.546214\n",
      "iter 31 loss is 39.546214\n",
      "iter 32 loss is 39.392918\n",
      "iter 33 loss is 39.392918\n",
      "iter 34 loss is 39.360285\n",
      "iter 35 loss is 39.137723\n",
      "iter 36 loss is 39.122535\n",
      "iter 37 loss is 39.079045\n",
      "iter 38 loss is 39.002605\n",
      "iter 39 loss is 39.002605\n",
      "iter 40 loss is 38.940534\n",
      "iter 41 loss is 38.940534\n",
      "iter 42 loss is 38.940534\n",
      "iter 43 loss is 38.803777\n",
      "iter 44 loss is 38.803777\n",
      "iter 45 loss is 38.803777\n",
      "iter 46 loss is 38.756414\n",
      "iter 47 loss is 38.756414\n",
      "iter 48 loss is 38.541580\n",
      "iter 49 loss is 38.454609\n",
      "iter 50 loss is 38.454609\n",
      "iter 51 loss is 38.454609\n",
      "iter 52 loss is 38.356769\n",
      "iter 53 loss is 38.356769\n",
      "iter 54 loss is 38.302835\n",
      "iter 55 loss is 38.234657\n",
      "iter 56 loss is 38.196011\n",
      "iter 57 loss is 38.196011\n",
      "iter 58 loss is 38.196011\n",
      "iter 59 loss is 38.130492\n",
      "iter 60 loss is 37.956739\n",
      "iter 61 loss is 37.790058\n",
      "iter 62 loss is 37.790058\n",
      "iter 63 loss is 37.790058\n",
      "iter 64 loss is 37.790058\n",
      "iter 65 loss is 37.790058\n",
      "iter 66 loss is 37.790058\n",
      "iter 67 loss is 37.776770\n",
      "iter 68 loss is 37.735432\n",
      "iter 69 loss is 37.703739\n",
      "iter 70 loss is 37.617174\n",
      "iter 71 loss is 37.617174\n",
      "iter 72 loss is 37.617174\n",
      "iter 73 loss is 37.617174\n",
      "iter 74 loss is 37.617174\n",
      "iter 75 loss is 37.592475\n",
      "iter 76 loss is 37.592475\n",
      "iter 77 loss is 37.592475\n",
      "iter 78 loss is 37.592475\n",
      "iter 79 loss is 37.502025\n",
      "iter 80 loss is 37.502025\n",
      "iter 81 loss is 37.311872\n",
      "iter 82 loss is 37.155484\n",
      "iter 83 loss is 37.114792\n",
      "iter 84 loss is 37.096336\n",
      "iter 85 loss is 37.096336\n",
      "iter 86 loss is 37.064943\n",
      "iter 87 loss is 37.038108\n",
      "iter 88 loss is 36.951179\n",
      "iter 89 loss is 36.951179\n",
      "iter 90 loss is 36.951179\n",
      "iter 91 loss is 36.951179\n",
      "iter 92 loss is 36.951179\n",
      "iter 93 loss is 36.951179\n",
      "iter 94 loss is 36.834874\n",
      "iter 95 loss is 36.834874\n",
      "iter 96 loss is 36.834874\n",
      "iter 97 loss is 36.806808\n",
      "iter 98 loss is 36.806808\n",
      "iter 99 loss is 36.806808\n",
      "iter 100 loss is 36.740212\n",
      "iter 101 loss is 36.740212\n",
      "iter 102 loss is 36.631380\n",
      "iter 103 loss is 36.631380\n",
      "iter 104 loss is 36.631380\n",
      "iter 105 loss is 36.631380\n",
      "iter 106 loss is 36.631380\n",
      "iter 107 loss is 36.631380\n",
      "iter 108 loss is 36.631380\n",
      "iter 109 loss is 36.631380\n",
      "iter 110 loss is 36.631380\n",
      "iter 111 loss is 36.631380\n",
      "iter 112 loss is 36.631380\n",
      "iter 113 loss is 36.415589\n",
      "iter 114 loss is 36.313949\n",
      "iter 115 loss is 36.313949\n",
      "iter 116 loss is 36.311410\n",
      "iter 117 loss is 36.311410\n",
      "iter 118 loss is 36.311410\n",
      "iter 119 loss is 36.311410\n",
      "iter 120 loss is 36.311410\n",
      "iter 121 loss is 36.310776\n",
      "iter 122 loss is 36.310776\n",
      "iter 123 loss is 36.310776\n",
      "iter 124 loss is 36.310776\n",
      "iter 125 loss is 36.310776\n",
      "iter 126 loss is 36.283249\n",
      "iter 127 loss is 36.283249\n",
      "iter 128 loss is 36.271756\n",
      "iter 129 loss is 36.270380\n",
      "iter 130 loss is 36.170599\n",
      "iter 131 loss is 36.170599\n",
      "iter 132 loss is 36.055381\n",
      "iter 133 loss is 36.055381\n",
      "iter 134 loss is 36.055381\n",
      "iter 135 loss is 35.948057\n",
      "iter 136 loss is 35.818924\n",
      "iter 137 loss is 35.731497\n",
      "iter 138 loss is 35.731497\n",
      "iter 139 loss is 35.703509\n",
      "iter 140 loss is 35.623866\n",
      "iter 141 loss is 35.623866\n",
      "iter 142 loss is 35.623866\n",
      "iter 143 loss is 35.514720\n",
      "iter 144 loss is 35.500622\n",
      "iter 145 loss is 35.500622\n",
      "iter 146 loss is 35.500622\n",
      "iter 147 loss is 35.436215\n",
      "iter 148 loss is 35.436215\n",
      "iter 149 loss is 35.254296\n",
      "iter 150 loss is 35.254296\n",
      "iter 151 loss is 35.167363\n",
      "iter 152 loss is 35.156115\n",
      "iter 153 loss is 35.031822\n",
      "iter 154 loss is 35.031822\n",
      "iter 155 loss is 35.031822\n",
      "iter 156 loss is 34.827623\n",
      "iter 157 loss is 34.827623\n",
      "iter 158 loss is 34.757844\n",
      "iter 159 loss is 34.757844\n",
      "iter 160 loss is 34.757844\n",
      "iter 161 loss is 34.757844\n",
      "iter 162 loss is 34.757844\n",
      "iter 163 loss is 34.757844\n",
      "iter 164 loss is 34.741455\n",
      "iter 165 loss is 34.741455\n",
      "iter 166 loss is 34.741455\n",
      "iter 167 loss is 34.694800\n",
      "iter 168 loss is 34.686288\n",
      "iter 169 loss is 34.686288\n",
      "iter 170 loss is 34.674554\n",
      "iter 171 loss is 34.674554\n",
      "iter 172 loss is 34.674554\n",
      "iter 173 loss is 34.622330\n",
      "iter 174 loss is 34.622330\n",
      "iter 175 loss is 34.516878\n",
      "iter 176 loss is 34.516878\n",
      "iter 177 loss is 34.401990\n",
      "iter 178 loss is 34.252150\n",
      "iter 179 loss is 34.252150\n",
      "iter 180 loss is 34.252150\n",
      "iter 181 loss is 34.252150\n",
      "iter 182 loss is 34.165679\n",
      "iter 183 loss is 34.165679\n",
      "iter 184 loss is 34.165679\n",
      "iter 185 loss is 34.165679\n",
      "iter 186 loss is 34.057531\n",
      "iter 187 loss is 34.019417\n",
      "iter 188 loss is 34.019417\n",
      "iter 189 loss is 34.019417\n",
      "iter 190 loss is 34.019417\n",
      "iter 191 loss is 33.941533\n",
      "iter 192 loss is 33.908469\n",
      "iter 193 loss is 33.882814\n",
      "iter 194 loss is 33.877752\n",
      "iter 195 loss is 33.877752\n",
      "iter 196 loss is 33.877752\n",
      "iter 197 loss is 33.877752\n",
      "iter 198 loss is 33.877752\n",
      "iter 199 loss is 33.877752\n",
      "iter 200 loss is 33.747902\n",
      "iter 201 loss is 33.738510\n",
      "iter 202 loss is 33.728427\n",
      "iter 203 loss is 33.728427\n",
      "iter 204 loss is 33.726943\n",
      "iter 205 loss is 33.726943\n",
      "iter 206 loss is 33.644991\n",
      "iter 207 loss is 33.644991\n",
      "iter 208 loss is 33.644991\n",
      "iter 209 loss is 33.633272\n",
      "iter 210 loss is 33.554718\n",
      "iter 211 loss is 33.499218\n",
      "iter 212 loss is 33.499218\n",
      "iter 213 loss is 33.431921\n",
      "iter 214 loss is 33.431921\n",
      "iter 215 loss is 33.398063\n",
      "iter 216 loss is 33.398063\n",
      "iter 217 loss is 33.330032\n",
      "iter 218 loss is 33.330032\n",
      "iter 219 loss is 33.330032\n",
      "iter 220 loss is 33.330032\n",
      "iter 221 loss is 33.330032\n",
      "iter 222 loss is 33.330032\n",
      "iter 223 loss is 33.303554\n",
      "iter 224 loss is 33.303554\n",
      "iter 225 loss is 33.303554\n",
      "iter 226 loss is 33.303554\n",
      "iter 227 loss is 33.303554\n",
      "iter 228 loss is 33.263945\n",
      "iter 229 loss is 33.263945\n",
      "iter 230 loss is 33.206785\n",
      "iter 231 loss is 33.135997\n",
      "iter 232 loss is 32.946689\n",
      "iter 233 loss is 32.946689\n",
      "iter 234 loss is 32.946689\n",
      "iter 235 loss is 32.913574\n",
      "iter 236 loss is 32.913574\n",
      "iter 237 loss is 32.913574\n",
      "iter 238 loss is 32.913574\n",
      "iter 239 loss is 32.859311\n",
      "iter 240 loss is 32.859311\n",
      "iter 241 loss is 32.785451\n",
      "iter 242 loss is 32.605310\n",
      "iter 243 loss is 32.537651\n",
      "iter 244 loss is 32.537651\n",
      "iter 245 loss is 32.537651\n",
      "iter 246 loss is 32.537651\n",
      "iter 247 loss is 32.292231\n",
      "iter 248 loss is 32.292231\n",
      "iter 249 loss is 32.292231\n",
      "iter 250 loss is 32.292231\n",
      "iter 251 loss is 32.292231\n",
      "iter 252 loss is 32.292231\n",
      "iter 253 loss is 32.270803\n",
      "iter 254 loss is 32.270803\n",
      "iter 255 loss is 32.270803\n",
      "iter 256 loss is 32.270803\n",
      "iter 257 loss is 32.263735\n",
      "iter 258 loss is 32.241612\n",
      "iter 259 loss is 32.192215\n",
      "iter 260 loss is 32.173110\n",
      "iter 261 loss is 32.099192\n",
      "iter 262 loss is 32.099192\n",
      "iter 263 loss is 32.079626\n",
      "iter 264 loss is 32.079626\n",
      "iter 265 loss is 31.998328\n",
      "iter 266 loss is 31.998328\n",
      "iter 267 loss is 31.902748\n",
      "iter 268 loss is 31.887725\n",
      "iter 269 loss is 31.887725\n",
      "iter 270 loss is 31.887725\n",
      "iter 271 loss is 31.887725\n",
      "iter 272 loss is 31.887725\n",
      "iter 273 loss is 31.849894\n",
      "iter 274 loss is 31.849894\n",
      "iter 275 loss is 31.849894\n",
      "iter 276 loss is 31.816144\n",
      "iter 277 loss is 31.715215\n",
      "iter 278 loss is 31.587003\n",
      "iter 279 loss is 31.411668\n",
      "iter 280 loss is 31.392813\n",
      "iter 281 loss is 31.385025\n",
      "iter 282 loss is 31.291429\n",
      "iter 283 loss is 31.187412\n",
      "iter 284 loss is 31.187412\n",
      "iter 285 loss is 31.187412\n",
      "iter 286 loss is 31.144231\n",
      "iter 287 loss is 31.144231\n",
      "iter 288 loss is 31.144231\n",
      "iter 289 loss is 31.144231\n",
      "iter 290 loss is 31.144231\n",
      "iter 291 loss is 31.105719\n",
      "iter 292 loss is 31.105719\n",
      "iter 293 loss is 31.022480\n",
      "iter 294 loss is 30.906454\n",
      "iter 295 loss is 30.906454\n",
      "iter 296 loss is 30.906454\n",
      "iter 297 loss is 30.906454\n",
      "iter 298 loss is 30.906454\n",
      "iter 299 loss is 30.906454\n",
      "iter 300 loss is 30.790767\n",
      "iter 301 loss is 30.790767\n",
      "iter 302 loss is 30.790767\n",
      "iter 303 loss is 30.726827\n",
      "iter 304 loss is 30.726827\n",
      "iter 305 loss is 30.726827\n",
      "iter 306 loss is 30.726827\n",
      "iter 307 loss is 30.726827\n",
      "iter 308 loss is 30.726827\n",
      "iter 309 loss is 30.725863\n",
      "iter 310 loss is 30.650175\n",
      "iter 311 loss is 30.650175\n",
      "iter 312 loss is 30.593794\n",
      "iter 313 loss is 30.583701\n",
      "iter 314 loss is 30.549852\n",
      "iter 315 loss is 30.349465\n",
      "iter 316 loss is 30.329815\n",
      "iter 317 loss is 30.212523\n",
      "iter 318 loss is 30.212523\n",
      "iter 319 loss is 30.171148\n",
      "iter 320 loss is 30.171148\n",
      "iter 321 loss is 30.147526\n",
      "iter 322 loss is 30.130489\n",
      "iter 323 loss is 30.130489\n",
      "iter 324 loss is 30.130489\n",
      "iter 325 loss is 30.023180\n",
      "iter 326 loss is 30.003168\n",
      "iter 327 loss is 29.951583\n",
      "iter 328 loss is 29.951583\n",
      "iter 329 loss is 29.951583\n",
      "iter 330 loss is 29.951583\n",
      "iter 331 loss is 29.793385\n",
      "iter 332 loss is 29.793385\n",
      "iter 333 loss is 29.793385\n",
      "iter 334 loss is 29.772640\n",
      "iter 335 loss is 29.772640\n",
      "iter 336 loss is 29.772640\n",
      "iter 337 loss is 29.752861\n",
      "iter 338 loss is 29.752861\n",
      "iter 339 loss is 29.752861\n",
      "iter 340 loss is 29.568078\n",
      "iter 341 loss is 29.526803\n",
      "iter 342 loss is 29.472453\n",
      "iter 343 loss is 29.393690\n",
      "iter 344 loss is 29.372038\n",
      "iter 345 loss is 29.310388\n",
      "iter 346 loss is 29.305292\n",
      "iter 347 loss is 29.305292\n",
      "iter 348 loss is 29.305292\n",
      "iter 349 loss is 29.270145\n",
      "iter 350 loss is 29.270145\n",
      "iter 351 loss is 29.270145\n",
      "iter 352 loss is 29.173409\n",
      "iter 353 loss is 29.137062\n",
      "iter 354 loss is 29.057557\n",
      "iter 355 loss is 29.057557\n",
      "iter 356 loss is 29.057557\n",
      "iter 357 loss is 28.853477\n",
      "iter 358 loss is 28.853477\n",
      "iter 359 loss is 28.789556\n",
      "iter 360 loss is 28.789556\n",
      "iter 361 loss is 28.789556\n",
      "iter 362 loss is 28.789556\n",
      "iter 363 loss is 28.665128\n",
      "iter 364 loss is 28.527579\n",
      "iter 365 loss is 28.500109\n",
      "iter 366 loss is 28.500109\n",
      "iter 367 loss is 28.500109\n",
      "iter 368 loss is 28.354654\n",
      "iter 369 loss is 28.354654\n",
      "iter 370 loss is 28.354654\n",
      "iter 371 loss is 28.354654\n",
      "iter 372 loss is 28.354654\n",
      "iter 373 loss is 28.251139\n",
      "iter 374 loss is 28.234808\n",
      "iter 375 loss is 28.195236\n",
      "iter 376 loss is 28.195236\n",
      "iter 377 loss is 28.178334\n",
      "iter 378 loss is 28.161472\n",
      "iter 379 loss is 28.161472\n",
      "iter 380 loss is 28.161472\n",
      "iter 381 loss is 28.014408\n",
      "iter 382 loss is 27.936997\n",
      "iter 383 loss is 27.936634\n",
      "iter 384 loss is 27.936634\n",
      "iter 385 loss is 27.923134\n",
      "iter 386 loss is 27.923134\n",
      "iter 387 loss is 27.923134\n",
      "iter 388 loss is 27.816294\n",
      "iter 389 loss is 27.816294\n",
      "iter 390 loss is 27.816294\n",
      "iter 391 loss is 27.675313\n",
      "iter 392 loss is 27.675313\n",
      "iter 393 loss is 27.654110\n",
      "iter 394 loss is 27.630776\n",
      "iter 395 loss is 27.521005\n",
      "iter 396 loss is 27.403927\n",
      "iter 397 loss is 27.356156\n",
      "iter 398 loss is 27.356156\n",
      "iter 399 loss is 27.326368\n",
      "iter 400 loss is 27.296192\n",
      "iter 401 loss is 27.197078\n",
      "iter 402 loss is 27.197078\n",
      "iter 403 loss is 27.197078\n",
      "iter 404 loss is 27.161499\n",
      "iter 405 loss is 27.132749\n",
      "iter 406 loss is 27.105681\n",
      "iter 407 loss is 27.105681\n",
      "iter 408 loss is 27.098339\n",
      "iter 409 loss is 27.098339\n",
      "iter 410 loss is 26.994732\n",
      "iter 411 loss is 26.994732\n",
      "iter 412 loss is 26.936460\n",
      "iter 413 loss is 26.845797\n",
      "iter 414 loss is 26.845797\n",
      "iter 415 loss is 26.831204\n",
      "iter 416 loss is 26.769997\n",
      "iter 417 loss is 26.736541\n",
      "iter 418 loss is 26.736541\n",
      "iter 419 loss is 26.703949\n",
      "iter 420 loss is 26.703949\n",
      "iter 421 loss is 26.703949\n",
      "iter 422 loss is 26.644066\n",
      "iter 423 loss is 26.642609\n",
      "iter 424 loss is 26.642609\n",
      "iter 425 loss is 26.642609\n",
      "iter 426 loss is 26.642609\n",
      "iter 427 loss is 26.600994\n",
      "iter 428 loss is 26.526742\n",
      "iter 429 loss is 26.508053\n",
      "iter 430 loss is 26.508053\n",
      "iter 431 loss is 26.407048\n",
      "iter 432 loss is 26.407048\n",
      "iter 433 loss is 26.407048\n",
      "iter 434 loss is 26.407048\n",
      "iter 435 loss is 26.407048\n",
      "iter 436 loss is 26.377203\n",
      "iter 437 loss is 26.344270\n",
      "iter 438 loss is 26.344270\n",
      "iter 439 loss is 26.239076\n",
      "iter 440 loss is 26.205405\n",
      "iter 441 loss is 26.205405\n",
      "iter 442 loss is 26.205405\n",
      "iter 443 loss is 26.205405\n",
      "iter 444 loss is 26.162349\n",
      "iter 445 loss is 26.132148\n",
      "iter 446 loss is 26.083628\n",
      "iter 447 loss is 26.006532\n",
      "iter 448 loss is 26.006532\n",
      "iter 449 loss is 26.006532\n",
      "iter 450 loss is 25.984784\n",
      "iter 451 loss is 25.963320\n",
      "iter 452 loss is 25.952565\n",
      "iter 453 loss is 25.952565\n",
      "iter 454 loss is 25.952565\n",
      "iter 455 loss is 25.934772\n",
      "iter 456 loss is 25.911690\n",
      "iter 457 loss is 25.911690\n",
      "iter 458 loss is 25.841716\n",
      "iter 459 loss is 25.761485\n",
      "iter 460 loss is 25.755235\n",
      "iter 461 loss is 25.755235\n",
      "iter 462 loss is 25.755235\n",
      "iter 463 loss is 25.712343\n",
      "iter 464 loss is 25.586286\n",
      "iter 465 loss is 25.586286\n",
      "iter 466 loss is 25.586286\n",
      "iter 467 loss is 25.554823\n",
      "iter 468 loss is 25.496826\n",
      "iter 469 loss is 25.413105\n",
      "iter 470 loss is 25.413105\n",
      "iter 471 loss is 25.413105\n",
      "iter 472 loss is 25.413105\n",
      "iter 473 loss is 25.395098\n",
      "iter 474 loss is 25.395098\n",
      "iter 475 loss is 25.395098\n",
      "iter 476 loss is 25.395098\n",
      "iter 477 loss is 25.395098\n",
      "iter 478 loss is 25.385495\n",
      "iter 479 loss is 25.265787\n",
      "iter 480 loss is 25.219589\n",
      "iter 481 loss is 25.164107\n",
      "iter 482 loss is 25.104419\n",
      "iter 483 loss is 25.104419\n",
      "iter 484 loss is 25.062571\n",
      "iter 485 loss is 25.062571\n",
      "iter 486 loss is 24.988044\n",
      "iter 487 loss is 24.988044\n",
      "iter 488 loss is 24.943635\n",
      "iter 489 loss is 24.943635\n",
      "iter 490 loss is 24.943635\n",
      "iter 491 loss is 24.943635\n",
      "iter 492 loss is 24.943635\n",
      "iter 493 loss is 24.906762\n",
      "iter 494 loss is 24.895579\n",
      "iter 495 loss is 24.895579\n",
      "iter 496 loss is 24.848517\n",
      "iter 497 loss is 24.848517\n",
      "iter 498 loss is 24.803858\n",
      "iter 499 loss is 24.803315\n",
      "iter 500 loss is 24.803315\n",
      "iter 501 loss is 24.747170\n",
      "iter 502 loss is 24.703498\n",
      "iter 503 loss is 24.650013\n",
      "iter 504 loss is 24.650013\n",
      "iter 505 loss is 24.650013\n",
      "iter 506 loss is 24.650013\n",
      "iter 507 loss is 24.645354\n",
      "iter 508 loss is 24.539969\n",
      "iter 509 loss is 24.492113\n",
      "iter 510 loss is 24.487652\n",
      "iter 511 loss is 24.487652\n",
      "iter 512 loss is 24.487652\n",
      "iter 513 loss is 24.487652\n",
      "iter 514 loss is 24.487652\n",
      "iter 515 loss is 24.487652\n",
      "iter 516 loss is 24.487652\n",
      "iter 517 loss is 24.444407\n",
      "iter 518 loss is 24.373969\n",
      "iter 519 loss is 24.373969\n",
      "iter 520 loss is 24.373969\n",
      "iter 521 loss is 24.352923\n",
      "iter 522 loss is 24.325988\n",
      "iter 523 loss is 24.325988\n",
      "iter 524 loss is 24.325988\n",
      "iter 525 loss is 24.325988\n",
      "iter 526 loss is 24.318731\n",
      "iter 527 loss is 24.318731\n",
      "iter 528 loss is 24.318731\n",
      "iter 529 loss is 24.292612\n",
      "iter 530 loss is 24.278936\n",
      "iter 531 loss is 24.275779\n",
      "iter 532 loss is 24.275779\n",
      "iter 533 loss is 24.275779\n",
      "iter 534 loss is 24.275779\n",
      "iter 535 loss is 24.201133\n",
      "iter 536 loss is 24.181133\n",
      "iter 537 loss is 24.181133\n",
      "iter 538 loss is 24.161729\n",
      "iter 539 loss is 24.121684\n",
      "iter 540 loss is 24.121684\n",
      "iter 541 loss is 24.121684\n",
      "iter 542 loss is 24.112748\n",
      "iter 543 loss is 24.068172\n",
      "iter 544 loss is 24.020365\n",
      "iter 545 loss is 24.020365\n",
      "iter 546 loss is 23.994318\n",
      "iter 547 loss is 23.994318\n",
      "iter 548 loss is 23.994318\n",
      "iter 549 loss is 23.989091\n",
      "iter 550 loss is 23.974475\n",
      "iter 551 loss is 23.940566\n",
      "iter 552 loss is 23.940566\n",
      "iter 553 loss is 23.940566\n",
      "iter 554 loss is 23.940566\n",
      "iter 555 loss is 23.901954\n",
      "iter 556 loss is 23.901954\n",
      "iter 557 loss is 23.876049\n",
      "iter 558 loss is 23.876049\n",
      "iter 559 loss is 23.865811\n",
      "iter 560 loss is 23.864932\n",
      "iter 561 loss is 23.826025\n",
      "iter 562 loss is 23.688622\n",
      "iter 563 loss is 23.688622\n",
      "iter 564 loss is 23.688622\n",
      "iter 565 loss is 23.688622\n",
      "iter 566 loss is 23.671386\n",
      "iter 567 loss is 23.671386\n",
      "iter 568 loss is 23.667054\n",
      "iter 569 loss is 23.667054\n",
      "iter 570 loss is 23.667054\n",
      "iter 571 loss is 23.635472\n",
      "iter 572 loss is 23.628762\n",
      "iter 573 loss is 23.628762\n",
      "iter 574 loss is 23.628762\n",
      "iter 575 loss is 23.583953\n",
      "iter 576 loss is 23.483385\n",
      "iter 577 loss is 23.483385\n",
      "iter 578 loss is 23.456469\n",
      "iter 579 loss is 23.375728\n",
      "iter 580 loss is 23.341119\n",
      "iter 581 loss is 23.323321\n",
      "iter 582 loss is 23.323321\n",
      "iter 583 loss is 23.323321\n",
      "iter 584 loss is 23.323321\n",
      "iter 585 loss is 23.323321\n",
      "iter 586 loss is 23.221292\n",
      "iter 587 loss is 23.221292\n",
      "iter 588 loss is 23.221292\n",
      "iter 589 loss is 23.221292\n",
      "iter 590 loss is 23.221292\n",
      "iter 591 loss is 23.147430\n",
      "iter 592 loss is 23.133346\n",
      "iter 593 loss is 23.133346\n",
      "iter 594 loss is 23.106090\n",
      "iter 595 loss is 23.106090\n",
      "iter 596 loss is 23.027422\n",
      "iter 597 loss is 23.027422\n",
      "iter 598 loss is 22.989715\n",
      "iter 599 loss is 22.989715\n",
      "iter 600 loss is 22.938294\n",
      "iter 601 loss is 22.855947\n",
      "iter 602 loss is 22.851465\n",
      "iter 603 loss is 22.851465\n",
      "iter 604 loss is 22.851465\n",
      "iter 605 loss is 22.851465\n",
      "iter 606 loss is 22.817704\n",
      "iter 607 loss is 22.817704\n",
      "iter 608 loss is 22.764285\n",
      "iter 609 loss is 22.713825\n",
      "iter 610 loss is 22.713825\n",
      "iter 611 loss is 22.713825\n",
      "iter 612 loss is 22.713825\n",
      "iter 613 loss is 22.696578\n",
      "iter 614 loss is 22.696578\n",
      "iter 615 loss is 22.696578\n",
      "iter 616 loss is 22.689151\n",
      "iter 617 loss is 22.689151\n",
      "iter 618 loss is 22.689151\n",
      "iter 619 loss is 22.644309\n",
      "iter 620 loss is 22.644309\n",
      "iter 621 loss is 22.556586\n",
      "iter 622 loss is 22.535343\n",
      "iter 623 loss is 22.505761\n",
      "iter 624 loss is 22.505761\n",
      "iter 625 loss is 22.505761\n",
      "iter 626 loss is 22.490671\n",
      "iter 627 loss is 22.490671\n",
      "iter 628 loss is 22.490671\n",
      "iter 629 loss is 22.465357\n",
      "iter 630 loss is 22.465357\n",
      "iter 631 loss is 22.397229\n",
      "iter 632 loss is 22.397229\n",
      "iter 633 loss is 22.397229\n",
      "iter 634 loss is 22.397229\n",
      "iter 635 loss is 22.368157\n",
      "iter 636 loss is 22.362133\n",
      "iter 637 loss is 22.319467\n",
      "iter 638 loss is 22.319467\n",
      "iter 639 loss is 22.314044\n",
      "iter 640 loss is 22.314044\n",
      "iter 641 loss is 22.314044\n",
      "iter 642 loss is 22.253746\n",
      "iter 643 loss is 22.233494\n",
      "iter 644 loss is 22.233494\n",
      "iter 645 loss is 22.233494\n",
      "iter 646 loss is 22.191979\n",
      "iter 647 loss is 22.191979\n",
      "iter 648 loss is 22.191979\n",
      "iter 649 loss is 22.141550\n",
      "iter 650 loss is 22.141550\n",
      "iter 651 loss is 22.052353\n",
      "iter 652 loss is 22.052353\n",
      "iter 653 loss is 22.052353\n",
      "iter 654 loss is 22.010539\n",
      "iter 655 loss is 21.937103\n",
      "iter 656 loss is 21.932309\n",
      "iter 657 loss is 21.886426\n",
      "iter 658 loss is 21.886426\n",
      "iter 659 loss is 21.886426\n",
      "iter 660 loss is 21.805465\n",
      "iter 661 loss is 21.796859\n",
      "iter 662 loss is 21.773700\n",
      "iter 663 loss is 21.773700\n",
      "iter 664 loss is 21.746857\n",
      "iter 665 loss is 21.746857\n",
      "iter 666 loss is 21.738845\n",
      "iter 667 loss is 21.738845\n",
      "iter 668 loss is 21.725234\n",
      "iter 669 loss is 21.670155\n",
      "iter 670 loss is 21.663561\n",
      "iter 671 loss is 21.642789\n",
      "iter 672 loss is 21.634057\n",
      "iter 673 loss is 21.588925\n",
      "iter 674 loss is 21.588925\n",
      "iter 675 loss is 21.577924\n",
      "iter 676 loss is 21.568923\n",
      "iter 677 loss is 21.568923\n",
      "iter 678 loss is 21.568923\n",
      "iter 679 loss is 21.502572\n",
      "iter 680 loss is 21.502572\n",
      "iter 681 loss is 21.502572\n",
      "iter 682 loss is 21.473172\n",
      "iter 683 loss is 21.473172\n",
      "iter 684 loss is 21.468749\n",
      "iter 685 loss is 21.449143\n",
      "iter 686 loss is 21.430213\n",
      "iter 687 loss is 21.417466\n",
      "iter 688 loss is 21.417038\n",
      "iter 689 loss is 21.417038\n",
      "iter 690 loss is 21.417038\n",
      "iter 691 loss is 21.417038\n",
      "iter 692 loss is 21.417038\n",
      "iter 693 loss is 21.417038\n",
      "iter 694 loss is 21.390737\n",
      "iter 695 loss is 21.390737\n",
      "iter 696 loss is 21.287059\n",
      "iter 697 loss is 21.283623\n",
      "iter 698 loss is 21.226810\n",
      "iter 699 loss is 21.226810\n",
      "iter 700 loss is 21.226810\n",
      "iter 701 loss is 21.226810\n",
      "iter 702 loss is 21.226810\n",
      "iter 703 loss is 21.191351\n",
      "iter 704 loss is 21.191351\n",
      "iter 705 loss is 21.174681\n",
      "iter 706 loss is 21.157909\n",
      "iter 707 loss is 21.157909\n",
      "iter 708 loss is 21.157909\n",
      "iter 709 loss is 21.157909\n",
      "iter 710 loss is 21.157909\n",
      "iter 711 loss is 21.151072\n",
      "iter 712 loss is 21.151072\n",
      "iter 713 loss is 21.151072\n",
      "iter 714 loss is 21.131579\n",
      "iter 715 loss is 21.131579\n",
      "iter 716 loss is 21.107675\n",
      "iter 717 loss is 21.107675\n",
      "iter 718 loss is 21.081592\n",
      "iter 719 loss is 21.041102\n",
      "iter 720 loss is 21.041102\n",
      "iter 721 loss is 21.041102\n",
      "iter 722 loss is 21.041102\n",
      "iter 723 loss is 21.040414\n",
      "iter 724 loss is 21.040414\n",
      "iter 725 loss is 21.040414\n",
      "iter 726 loss is 21.040414\n",
      "iter 727 loss is 21.003149\n",
      "iter 728 loss is 21.003149\n",
      "iter 729 loss is 20.968555\n",
      "iter 730 loss is 20.968555\n",
      "iter 731 loss is 20.968555\n",
      "iter 732 loss is 20.958946\n",
      "iter 733 loss is 20.958946\n",
      "iter 734 loss is 20.938366\n",
      "iter 735 loss is 20.938366\n",
      "iter 736 loss is 20.938366\n",
      "iter 737 loss is 20.938366\n",
      "iter 738 loss is 20.931350\n",
      "iter 739 loss is 20.906583\n",
      "iter 740 loss is 20.906583\n",
      "iter 741 loss is 20.889945\n",
      "iter 742 loss is 20.851503\n",
      "iter 743 loss is 20.850252\n",
      "iter 744 loss is 20.844151\n",
      "iter 745 loss is 20.844151\n",
      "iter 746 loss is 20.843167\n",
      "iter 747 loss is 20.836520\n",
      "iter 748 loss is 20.836520\n",
      "iter 749 loss is 20.836520\n",
      "iter 750 loss is 20.836520\n",
      "iter 751 loss is 20.760794\n",
      "iter 752 loss is 20.759054\n",
      "iter 753 loss is 20.701196\n",
      "iter 754 loss is 20.701196\n",
      "iter 755 loss is 20.701196\n",
      "iter 756 loss is 20.701196\n",
      "iter 757 loss is 20.690015\n",
      "iter 758 loss is 20.683111\n",
      "iter 759 loss is 20.670394\n",
      "iter 760 loss is 20.670394\n",
      "iter 761 loss is 20.670394\n",
      "iter 762 loss is 20.670394\n",
      "iter 763 loss is 20.655634\n",
      "iter 764 loss is 20.655634\n",
      "iter 765 loss is 20.653077\n",
      "iter 766 loss is 20.653077\n",
      "iter 767 loss is 20.653077\n",
      "iter 768 loss is 20.653077\n",
      "iter 769 loss is 20.653077\n",
      "iter 770 loss is 20.653077\n",
      "iter 771 loss is 20.653077\n",
      "iter 772 loss is 20.633357\n",
      "iter 773 loss is 20.633357\n",
      "iter 774 loss is 20.605606\n",
      "iter 775 loss is 20.605606\n",
      "iter 776 loss is 20.580069\n",
      "iter 777 loss is 20.580069\n",
      "iter 778 loss is 20.567701\n",
      "iter 779 loss is 20.551838\n",
      "iter 780 loss is 20.539605\n",
      "iter 781 loss is 20.539605\n",
      "iter 782 loss is 20.539605\n",
      "iter 783 loss is 20.538153\n",
      "iter 784 loss is 20.538153\n",
      "iter 785 loss is 20.538153\n",
      "iter 786 loss is 20.538153\n",
      "iter 787 loss is 20.538153\n",
      "iter 788 loss is 20.530500\n",
      "iter 789 loss is 20.530500\n",
      "iter 790 loss is 20.530500\n",
      "iter 791 loss is 20.530500\n",
      "iter 792 loss is 20.530500\n",
      "iter 793 loss is 20.514206\n",
      "iter 794 loss is 20.496487\n",
      "iter 795 loss is 20.482720\n",
      "iter 796 loss is 20.453487\n",
      "iter 797 loss is 20.453487\n",
      "iter 798 loss is 20.415326\n",
      "iter 799 loss is 20.409333\n",
      "iter 800 loss is 20.409333\n",
      "iter 801 loss is 20.409333\n",
      "iter 802 loss is 20.409333\n",
      "iter 803 loss is 20.388723\n",
      "iter 804 loss is 20.388723\n",
      "iter 805 loss is 20.388723\n",
      "iter 806 loss is 20.388723\n",
      "iter 807 loss is 20.323520\n",
      "iter 808 loss is 20.323520\n",
      "iter 809 loss is 20.323520\n",
      "iter 810 loss is 20.311830\n",
      "iter 811 loss is 20.311830\n",
      "iter 812 loss is 20.311830\n",
      "iter 813 loss is 20.286908\n",
      "iter 814 loss is 20.267403\n",
      "iter 815 loss is 20.267403\n",
      "iter 816 loss is 20.267403\n",
      "iter 817 loss is 20.225888\n",
      "iter 818 loss is 20.216392\n",
      "iter 819 loss is 20.208792\n",
      "iter 820 loss is 20.208792\n",
      "iter 821 loss is 20.208792\n",
      "iter 822 loss is 20.206488\n",
      "iter 823 loss is 20.206059\n",
      "iter 824 loss is 20.200241\n",
      "iter 825 loss is 20.192521\n",
      "iter 826 loss is 20.163047\n",
      "iter 827 loss is 20.163047\n",
      "iter 828 loss is 20.163047\n",
      "iter 829 loss is 20.135858\n",
      "iter 830 loss is 20.135858\n",
      "iter 831 loss is 20.135858\n",
      "iter 832 loss is 20.135858\n",
      "iter 833 loss is 20.135858\n",
      "iter 834 loss is 20.135858\n",
      "iter 835 loss is 20.127059\n",
      "iter 836 loss is 20.118742\n",
      "iter 837 loss is 20.068978\n",
      "iter 838 loss is 20.022489\n",
      "iter 839 loss is 19.997897\n",
      "iter 840 loss is 19.988404\n",
      "iter 841 loss is 19.988404\n",
      "iter 842 loss is 19.963599\n",
      "iter 843 loss is 19.963599\n",
      "iter 844 loss is 19.946119\n",
      "iter 845 loss is 19.896505\n",
      "iter 846 loss is 19.896505\n",
      "iter 847 loss is 19.896505\n",
      "iter 848 loss is 19.886657\n",
      "iter 849 loss is 19.886657\n",
      "iter 850 loss is 19.886657\n",
      "iter 851 loss is 19.825159\n",
      "iter 852 loss is 19.825159\n",
      "iter 853 loss is 19.823092\n",
      "iter 854 loss is 19.797890\n",
      "iter 855 loss is 19.797890\n",
      "iter 856 loss is 19.779548\n",
      "iter 857 loss is 19.772430\n",
      "iter 858 loss is 19.755624\n",
      "iter 859 loss is 19.755624\n",
      "iter 860 loss is 19.755624\n",
      "iter 861 loss is 19.755624\n",
      "iter 862 loss is 19.740821\n",
      "iter 863 loss is 19.721678\n",
      "iter 864 loss is 19.686646\n",
      "iter 865 loss is 19.655115\n",
      "iter 866 loss is 19.655115\n",
      "iter 867 loss is 19.638638\n",
      "iter 868 loss is 19.638638\n",
      "iter 869 loss is 19.638638\n",
      "iter 870 loss is 19.638638\n",
      "iter 871 loss is 19.638638\n",
      "iter 872 loss is 19.638638\n",
      "iter 873 loss is 19.630685\n",
      "iter 874 loss is 19.630685\n",
      "iter 875 loss is 19.630685\n",
      "iter 876 loss is 19.630685\n",
      "iter 877 loss is 19.617288\n",
      "iter 878 loss is 19.617288\n",
      "iter 879 loss is 19.617288\n",
      "iter 880 loss is 19.600157\n",
      "iter 881 loss is 19.600157\n",
      "iter 882 loss is 19.591551\n",
      "iter 883 loss is 19.588853\n",
      "iter 884 loss is 19.588853\n",
      "iter 885 loss is 19.588853\n",
      "iter 886 loss is 19.588853\n",
      "iter 887 loss is 19.568347\n",
      "iter 888 loss is 19.565656\n",
      "iter 889 loss is 19.538405\n",
      "iter 890 loss is 19.538405\n",
      "iter 891 loss is 19.514602\n",
      "iter 892 loss is 19.484473\n",
      "iter 893 loss is 19.484473\n",
      "iter 894 loss is 19.484473\n",
      "iter 895 loss is 19.484473\n",
      "iter 896 loss is 19.484473\n",
      "iter 897 loss is 19.459412\n",
      "iter 898 loss is 19.459412\n",
      "iter 899 loss is 19.455350\n",
      "iter 900 loss is 19.439937\n",
      "iter 901 loss is 19.439937\n",
      "iter 902 loss is 19.439937\n",
      "iter 903 loss is 19.439937\n",
      "iter 904 loss is 19.437221\n",
      "iter 905 loss is 19.437221\n",
      "iter 906 loss is 19.436994\n",
      "iter 907 loss is 19.436994\n",
      "iter 908 loss is 19.436994\n",
      "iter 909 loss is 19.415791\n",
      "iter 910 loss is 19.415791\n",
      "iter 911 loss is 19.415791\n",
      "iter 912 loss is 19.415060\n",
      "iter 913 loss is 19.415060\n",
      "iter 914 loss is 19.415060\n",
      "iter 915 loss is 19.415060\n",
      "iter 916 loss is 19.415060\n",
      "iter 917 loss is 19.415060\n",
      "iter 918 loss is 19.411169\n",
      "iter 919 loss is 19.411014\n",
      "iter 920 loss is 19.411014\n",
      "iter 921 loss is 19.411014\n",
      "iter 922 loss is 19.397120\n",
      "iter 923 loss is 19.397120\n",
      "iter 924 loss is 19.394325\n",
      "iter 925 loss is 19.365437\n",
      "iter 926 loss is 19.350721\n",
      "iter 927 loss is 19.350721\n",
      "iter 928 loss is 19.350721\n",
      "iter 929 loss is 19.324628\n",
      "iter 930 loss is 19.324628\n",
      "iter 931 loss is 19.324628\n",
      "iter 932 loss is 19.324628\n",
      "iter 933 loss is 19.324628\n",
      "iter 934 loss is 19.324628\n",
      "iter 935 loss is 19.324339\n",
      "iter 936 loss is 19.324339\n",
      "iter 937 loss is 19.324339\n",
      "iter 938 loss is 19.301811\n",
      "iter 939 loss is 19.288057\n",
      "iter 940 loss is 19.276484\n",
      "iter 941 loss is 19.276484\n",
      "iter 942 loss is 19.273748\n",
      "iter 943 loss is 19.273748\n",
      "iter 944 loss is 19.255755\n",
      "iter 945 loss is 19.255755\n",
      "iter 946 loss is 19.255755\n",
      "iter 947 loss is 19.255755\n",
      "iter 948 loss is 19.255755\n",
      "iter 949 loss is 19.255755\n",
      "iter 950 loss is 19.255755\n",
      "iter 951 loss is 19.255755\n",
      "iter 952 loss is 19.248599\n",
      "iter 953 loss is 19.221247\n",
      "iter 954 loss is 19.221247\n",
      "iter 955 loss is 19.221247\n",
      "iter 956 loss is 19.205184\n",
      "iter 957 loss is 19.205184\n",
      "iter 958 loss is 19.205184\n",
      "iter 959 loss is 19.189923\n",
      "iter 960 loss is 19.189923\n",
      "iter 961 loss is 19.189923\n",
      "iter 962 loss is 19.189923\n",
      "iter 963 loss is 19.184574\n",
      "iter 964 loss is 19.184574\n",
      "iter 965 loss is 19.165479\n",
      "iter 966 loss is 19.165479\n",
      "iter 967 loss is 19.165479\n",
      "iter 968 loss is 19.165479\n",
      "iter 969 loss is 19.090447\n",
      "iter 970 loss is 19.075051\n",
      "iter 971 loss is 19.075051\n",
      "iter 972 loss is 19.074096\n",
      "iter 973 loss is 19.074096\n",
      "iter 974 loss is 19.074096\n",
      "iter 975 loss is 19.074096\n",
      "iter 976 loss is 19.074096\n",
      "iter 977 loss is 19.073310\n",
      "iter 978 loss is 19.073310\n",
      "iter 979 loss is 19.058264\n",
      "iter 980 loss is 19.058264\n",
      "iter 981 loss is 19.058264\n",
      "iter 982 loss is 19.051651\n",
      "iter 983 loss is 19.040849\n",
      "iter 984 loss is 19.025626\n",
      "iter 985 loss is 19.025626\n",
      "iter 986 loss is 19.007034\n",
      "iter 987 loss is 19.007034\n",
      "iter 988 loss is 19.006829\n",
      "iter 989 loss is 18.986148\n",
      "iter 990 loss is 18.980069\n",
      "iter 991 loss is 18.980069\n",
      "iter 992 loss is 18.980069\n",
      "iter 993 loss is 18.980069\n",
      "iter 994 loss is 18.963937\n",
      "iter 995 loss is 18.952016\n",
      "iter 996 loss is 18.952016\n",
      "iter 997 loss is 18.952016\n",
      "iter 998 loss is 18.934314\n",
      "iter 999 loss is 18.934314\n"
     ]
    }
   ],
   "source": [
    "#随机下降法\n",
    "W = np.random.randn(10, 3073) * 0.001 # generate random starting W\n",
    "bestloss = float(\"inf\")\n",
    "for i in range(1000):\n",
    "    step_size = 0.00001\n",
    "    Wtry = W + np.random.randn(10, 3073) * step_size\n",
    "    loss = L(X_train, Y_train, Wtry)\n",
    "    if loss < bestloss:\n",
    "        W = Wtry\n",
    "        bestloss = loss\n",
    "    print('iter %d loss is %f' % (i, bestloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 梯度下降法\n",
    "def eval_numerical_gradient(f, x):\n",
    "    \"\"\" \n",
    "    a naive implementation of numerical gradient of f at x \n",
    "    - f should be a function that takes a single argument\n",
    "    - x is the point (numpy array) to evaluate the gradient at\n",
    "    \"\"\" \n",
    "\n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros(x.shape)\n",
    "    h = 0.00001\n",
    "\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        old_value = x[ix]\n",
    "        x[ix] = old_value + h # increment by h\n",
    "        fxh = f(x) # evalute f(x + h)\n",
    "        x[ix] = old_value # restore to previous value (very important!)\n",
    "\n",
    "        # compute the partial derivative\n",
    "        grad[ix] = (fxh - fx) / h # the slope\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad\n",
    "\n",
    "# 10000个太多了,测试1000个\n",
    "k=100\n",
    "def CIFAR10_loss_fun(W):\n",
    "    return L(X_train[:k], Y_train[:k], W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.6 s, sys: 216 ms, total: 18.8 s\n",
      "Wall time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "W = np.random.rand(10, 3073) * 0.001 # random weight vector\n",
    "\n",
    "df = eval_numerical_gradient(CIFAR10_loss_fun, W) # get the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original loss: 18.754741\n",
      "for step size 0.000000 new loss: 18.740895\n",
      "for step size 0.000000 new loss: 18.616471\n",
      "for step size 0.000000 new loss: 17.402123\n",
      "for step size 0.000000 new loss: 10.438134\n",
      "for step size 0.000001 new loss: 124.150568\n",
      "for step size 0.000010 new loss: 1328.548602\n",
      "for step size 0.000100 new loss: 13372.797059\n",
      "for step size 0.001000 new loss: 133815.281625\n",
      "for step size 0.010000 new loss: 1338240.127288\n",
      "for step size 0.100000 new loss: 13382488.583912\n",
      "CPU times: user 15.1 ms, sys: 2.49 ms, total: 17.6 ms\n",
      "Wall time: 15.2 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "loss_original = CIFAR10_loss_fun(W) # the original loss\n",
    "print('original loss: %f' % (loss_original, ))\n",
    "\n",
    "for step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]:\n",
    "    step_size = 10 ** step_size_log\n",
    "    W_new = W - step_size * df # new position in the weight space\n",
    "    loss_new = CIFAR10_loss_fun(W_new)\n",
    "    print('for step size %f new loss: %f' % (step_size, loss_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**结论:**要选择大小合适的stepsize\n",
    "\n",
    "- 梯度方向其实是函数增加最快的方向\n",
    "- 梯度下降法是沿着负梯度方向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
